export default {
  id: 'quiz-ai-003',
  title: '인공신경망 기초 다지기',
  description: 'Basic Artificial Neural Networks에 대한 퀴즈입니다. 해당 포스팅을 참고하여 문제를 풀어보세요.',
  author: 'Hong',
  category: '딥러닝',
  points: 100,
  createdAt: '2025-11-20',
  questions: [
    {
      id: 201,
      text: '인공신경망의 가장 기본 단위인 "퍼셉트론(Perceptron)"의 구성 요소가 아닌 것은?',
      options: ['입력값 (Input)', '가중치 (Weight)', '편향 (Bias)', '순환 연결 (Recurrent Connection)'],
      answer: 3,
      shortExplanation: '순환 연결은 RNN의 특징이며, 기본 퍼셉트론의 구성 요소가 아닙니다.',
      detailedExplanation: '단층 퍼셉트론은 입력값(x), 가중치(w), 편향(b), 그리고 활성화 함수로 구성됩니다. 순환 연결(Recurrent Connection)은 이전 단계의 출력이 다시 입력으로 들어오는 구조로, 주로 순환신경망(RNN)에서 사용됩니다.',
    },
    {
      id: 202,
      text: '초기 단층 퍼셉트론이 해결하지 못해 "AI의 첫 번째 겨울"을 불러왔던 논리 회로 문제는 무엇인가요?',
      options: ['AND 게이트', 'OR 게이트', 'XOR 게이트', 'NOT 게이트'],
      answer: 2,
      shortExplanation: '단층 퍼셉트론은 선형 분리가 불가능한 XOR 게이트 문제를 해결할 수 없었습니다.',
      detailedExplanation: 'AND나 OR 게이트는 직선 하나로 데이터를 분류할 수 있는 선형 문제이지만, XOR 게이트는 직선 하나로 0과 1을 구분할 수 없는 비선형 문제입니다. 이를 해결하기 위해 나중에 다층 퍼셉트론(MLP)이 등장하게 됩니다.',
    },
    {
      id: 203,
      text: '신경망에서 "활성화 함수(Activation Function)"를 사용하는 가장 주된 이유는 무엇인가요?',
      options: ['계산 속도를 높이기 위해', '데이터에 비선형성(Non-linearity)을 부여하기 위해', '입력 데이터의 차원을 줄이기 위해', '오버피팅을 방지하기 위해'],
      answer: 1,
      shortExplanation: '활성화 함수가 없으면 신경망은 깊게 쌓아도 하나의 선형 함수와 다를 바가 없습니다.',
      detailedExplanation: '활성화 함수(Sigmoid, ReLU 등)를 사용함으로써 신경망은 복잡한 비선형 패턴을 학습할 수 있게 됩니다. 이것이 없다면 아무리 층을 깊게 쌓아도 결국은 단순한 선형 결합($Wx+b$)으로 표현되어 복잡한 문제를 풀 수 없습니다.',
    },
    {
      id: 204,
      image: 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/ArtificialNeuronModel_english.png/640px-ArtificialNeuronModel_english.png',
      text: '다음 중 "가중치(Weight)"의 역할에 대한 설명으로 가장 적절한 것은?',
      options: ['입력 신호가 출력에 미치는 중요도를 결정한다.', '뉴런이 얼마나 쉽게 활성화될지를 조정한다.', '데이터의 순서를 결정한다.', '항상 0과 1 사이의 값만 가진다.'],
      answer: 0,
      shortExplanation: '가중치는 입력 신호의 강도나 중요도를 조절하는 매개변수입니다.',
      detailedExplanation: '가중치($w$)는 입력값($x$)에 곱해지는 값으로, 해당 입력 신호가 결과에 얼마나 큰 영향을 미칠지를 결정합니다. 반면, 뉴런의 활성화 난이도를 조절하는 것은 편향(Bias, $b$)의 역할입니다.',
    },
    {
      id: 205,
      text: '다층 퍼셉트론(MLP)의 학습 과정인 "역전파(Backpropagation)" 알고리즘의 핵심 원리는 무엇인가요?',
      options: ['입력에서 출력 방향으로 오차를 전송한다.', '출력단의 오차를 미분을 통해 입력 방향으로 전파하며 가중치를 갱신한다.', '무작위로 가중치를 변경하며 최적의 값을 찾는다.', '모든 가중치를 0으로 초기화한 후 학습한다.'],
      answer: 1,
      shortExplanation: '역전파는 연쇄 법칙(Chain Rule)을 이용해 오차를 뒤에서 앞으로 전파하며 학습합니다.',
      detailedExplanation: '역전파 알고리즘은 출력값과 정답 사이의 오차(Loss)를 구한 뒤, 이를 줄이는 방향으로(경사하강법) 출력층에서 입력층 방향으로 거슬러 올라가며 각 가중치를 업데이트하는 방식입니다.',
    },
    {
      id: 206,
      text: '다음 중 은닉층(Hidden Layer)이 깊어질수록 Sigmoid 함수보다 ReLU 함수를 선호하는 이유는 무엇인가요?',
      options: ['ReLU가 Sigmoid보다 구현하기 복잡해서', 'ReLU는 음수 값을 더 잘 처리하기 때문에', 'Sigmoid의 기울기 소실(Vanishing Gradient) 문제를 해결하기 위해', 'ReLU는 항상 0과 1 사이의 값을 출력하기 때문에'],
      answer: 2,
      shortExplanation: 'Sigmoid는 양 끝단에서 미분값이 0에 가까워져 학습이 멈추는 기울기 소실 문제가 발생합니다.',
      detailedExplanation: 'Sigmoid 함수는 입력값이 크거나 작을 때 기울기(미분값)가 거의 0이 되어, 역전파 과정에서 가중치 업데이트가 사라지는 "기울기 소실(Vanishing Gradient)" 문제가 발생합니다. ReLU는 양수 구간에서 기울기가 항상 1이므로 이 문제를 완화할 수 있습니다.',
    },
    {
      id: 207,
      text: '딥러닝 학습 시, 전체 데이터를 한 번 다 학습하는 주기를 무엇이라고 하나요?',
      options: ['Batch (배치)', 'Iteration (이터레이션)', 'Epoch (에포크)', 'Step (스텝)'],
      answer: 2,
      shortExplanation: 'Epoch는 전체 데이터셋에 대해 한 번 학습을 완료한 상태를 의미합니다.',
      detailedExplanation: '1 Epoch는 전체 트레이닝 데이터가 신경망을 한 번 통과한 것을 의미합니다. Batch Size는 한 번의 가중치 업데이트에 사용되는 데이터의 양을 의미하며, Iteration은 1 Epoch를 마치기 위해 필요한 배치의 개수를 의미합니다.',
    },
    {
      id: 208,
      text: '손실 함수(Loss Function)의 역할은 무엇인가요?',
      options: ['모델의 정확도를 높여주는 함수이다.', '모델의 예측값과 실제 정답 사이의 차이를 계산하는 함수이다.', '데이터를 정규화(Normalization)하는 함수이다.', '학습 속도를 결정하는 함수이다.'],
      answer: 1,
      shortExplanation: '손실 함수는 모델이 얼마나 틀렸는지를 수치화하여 나타냅니다.',
      detailedExplanation: '손실 함수(비용 함수)는 현재 모델의 예측값($\hat{y}$)과 실제값($y$)의 차이를 계산합니다. 학습의 목표는 이 손실 함수의 값을 최소화하는 가중치를 찾는 것입니다. 대표적으로 MSE(평균제곱오차)와 Cross-Entropy가 있습니다.',
    }
  ]
};