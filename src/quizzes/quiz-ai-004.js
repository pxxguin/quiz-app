export default {
  id: 'quiz-ai-004',
  title: '오차 역전파 정복하기',
  description: '딥러닝 학습의 핵심 메커니즘인 오차 역전파 알고리즘에 대한 퀴즈입니다. 해당 포스팅을 참고하여 문제를 풀어보세요.',
  author: 'Hong',
  category: '딥러닝',
  points: 100,
  createdAt: '2025-11-20',
  questions: [
    {
      id: 301,
      text: '오차 역전파 알고리즘의 주된 목적은 무엇인가요?',
      options: ['입력 데이터를 변환하여 정답을 예측하기 위해', '손실(Loss)에 대한 각 가중치의 미분값(기울기)을 효율적으로 구하기 위해', '데이터의 잡음(Noise)을 제거하기 위해', '신경망의 구조를 자동으로 설계하기 위해'],
      answer: 1,
      shortExplanation: '역전파는 손실 함수를 최소화하기 위해 각 파라미터가 얼마나 변화해야 하는지(기울기)를 계산하는 과정입니다.',
      detailedExplanation: '순전파가 예측값을 구하는 과정이라면, 역전파는 정답과 예측값의 차이(Loss)를 바탕으로 각 가중치가 오차에 기여한 정도(Gradient)를 출력층에서 입력층 방향으로 계산해내는 과정입니다.',
    },
    {
      id: 302,
      text: '오차 역전파 알고리즘의 수학적 기반이 되는 핵심 원리는 무엇인가요?',
      options: ['피타고라스 정리', '연쇄 법칙 (Chain Rule)', '중심극한정리', '베이즈 정리'],
      answer: 1,
      shortExplanation: '합성 함수의 미분을 구하기 위해 연쇄 법칙을 사용합니다.',
      detailedExplanation: '신경망은 수많은 함수가 중첩된 합성 함수 형태입니다. 깊은 층의 미분값을 구하기 위해서는 "겉미분 × 속미분"으로 알려진 연쇄 법칙(Chain Rule)을 사용하여 국소적인 미분값들을 곱해나가는 방식을 사용합니다.',
    },
    {
      id: 303,
      // image: 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/StochasticGradientDescent.png/440px-StochasticGradientDescent.png',
      text: '경사 하강법(Gradient Descent)에서 가중치($W$)를 업데이트하는 수식으로 올바른 것은? ($\\eta$: 학습률, $\\nabla E$: 기울기)',
      options: ['$W_{new} = W_{old} + \\eta \\cdot \\nabla E$', '$W_{new} = W_{old} - \\eta \\cdot \\nabla E$', '$W_{new} = W_{old} \\times \\eta \\cdot \\nabla E$', '$W_{new} = \\nabla E - \\eta \\cdot W_{old}$'],
      answer: 1,
      shortExplanation: '오차를 줄이기 위해서는 기울기의 반대 방향으로 이동해야 하므로 뺄셈(-)을 합니다.',
      detailedExplanation: '기울기($\\nabla E$)는 오차가 증가하는 방향을 가리킵니다. 우리는 오차를 최소화해야 하므로, 기울기가 가리키는 반대 방향(내리막길)으로 가중치를 이동시켜야 합니다. 따라서 현재 가중치에서 기울기에 학습률을 곱한 값을 뺍니다.',
    },
    {
      id: 304,
      text: '학습률(Learning Rate)이 너무 클 때 발생할 수 있는 문제는 무엇인가요?',
      options: ['학습 속도가 너무 느려진다.', '로컬 미니멈(Local Minimum)에 갇히기 쉽다.', '최적점(Global Minimum)을 지나쳐 발산(Overshooting)할 수 있다.', '과적합(Overfitting)이 무조건 발생한다.'],
      answer: 2,
      shortExplanation: '보폭(학습률)이 너무 크면 최적의 지점에 도달하지 못하고 건너뛰거나 튕겨 나갈 수 있습니다.',
      detailedExplanation: '학습률은 한 번에 얼마나 이동할지를 결정하는 보폭과 같습니다. 보폭이 너무 크면 골짜기(최소 오차 지점)의 바닥에 안착하지 못하고 반대편으로 튀어 오르거나 영원히 수렴하지 못하는 문제가 발생할 수 있습니다.',
    },
    {
      id: 305,
      text: '시그모이드(Sigmoid) 함수를 깊은 신경망의 은닉층에 사용할 때 발생하는 치명적인 문제는 무엇인가요?',
      options: ['기울기 소실 (Vanishing Gradient)', '기울기 폭주 (Exploding Gradient)', '메모리 부족 (Out of Memory)', '데드 렐루 (Dying ReLU)'],
      answer: 0,
      shortExplanation: '시그모이드의 미분값은 최대 0.25에 불과해, 층이 깊어질수록 기울기가 0에 수렴합니다.',
      detailedExplanation: '역전파 과정에서 연쇄 법칙에 의해 미분값들이 계속 곱해지는데, 시그모이드 함수의 도함수 값은 0과 0.25 사이입니다. 1보다 작은 값을 계속 곱하면 결과가 0에 가까워져, 입력층 근처의 가중치들이 업데이트되지 않는 기울기 소실 문제가 발생합니다.',
    },
    {
      id: 306,
      text: '다음 중 "계산 그래프(Computational Graph)"를 이용해 역전파를 설명할 때, "덧셈 노드(+)"의 역전파 특징은 무엇인가요?',
      options: ['상류에서 전해진 미분값을 그대로 흘려보낸다.', '상류에서 전해진 미분값에 입력 신호를 곱해서 흘려보낸다.', '상류에서 전해진 미분값의 부호를 바꾼다.', '입력 신호들을 서로 바꾼 뒤 미분값을 곱한다.'],
      answer: 0,
      shortExplanation: '덧셈 노드의 미분은 1이므로, 상류의 미분값을 그대로 하류로 전달합니다.',
      detailedExplanation: '$z = x + y$일 때, $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = 1$입니다. 따라서 연쇄 법칙 적용 시 상류의 미분값에 1을 곱하게 되므로, 값의 변화 없이 그대로 역전파됩니다.',
    },
    {
      id: 307,
      text: '역전파 알고리즘의 수행 순서로 올바른 것은?',
      options: ['가중치 초기화 → 역전파 → 순전파 → 오차 계산', '가중치 초기화 → 순전파 → 오차 계산 → 역전파 & 가중치 갱신', '오차 계산 → 순전파 → 역전파 → 가중치 초기화', '순전파 → 가중치 갱신 → 역전파 → 오차 계산'],
      answer: 1,
      shortExplanation: '먼저 예측을 하고(순전파), 틀린 정도를 잰 뒤(오차), 피드백을 반영(역전파)합니다.',
      detailedExplanation: '학습은 1) 입력을 넣어 예측값을 얻는 순전파, 2) 실제값과 비교하여 손실(Loss) 계산, 3) 손실에 대한 기울기를 구하는 역전파, 4) 구한 기울기로 가중치를 업데이트하는 단계(Optimizer)로 반복됩니다.',
    },
    {
      id: 308,
      text: '행렬 곱셈(MatMul) 노드의 역전파 과정에서 필수로 고려해야 하는 것은 무엇인가요?',
      options: ['행렬의 원소 합', '행렬의 형상(Shape) 일치 및 전치(Transpose)', '행렬의 역행렬 존재 여부', '행렬의 대각 성분'],
      answer: 1,
      shortExplanation: '행렬 곱의 미분 계산 시 차원을 맞추기 위해 전치 행렬($W^T$)을 사용해야 합니다.',
      detailedExplanation: '행렬 연산의 역전파에서는 기울기 행렬의 형상(Shape)이 순전파 때의 입력 형상과 일치해야 합니다. 이를 위해 서로 곱해진 행렬의 순서를 바꾸거나 전치(Transpose, $T$)를 취하여 내적을 구하는 과정이 필요합니다.',
    }
  ]
};