export default {
  id: 'quiz-ai-006',
  title: 'Optimizer의 종류와 특징',
  description: 'What is optimizer and how they work?에 대한 퀴즈입니다. 해당 포스팅을 참고하여 문제를 풀어보세요.',
  author: 'Hong',
  category: '딥러닝',
  points: 160,
  createdAt: '2025-11-24',
  questions: [
    {
      id: 601,
      text: '모델이 예측한 값과 실제 값 사이의 차이를 구하는 함수로, 이 값을 최소화하는 것이 학습의 목표가 되는 함수는 무엇인가요?',
      options: ['활성화 함수', '손실 함수', '임베딩 함수', '전이 함수'],
      answer: 1,
      shortExplanation: '정답은 손실 함수입니다.',
      detailedExplanation: '손실 함수는 모델의 예측값과 실제값의 오차를 계산하는 함수이며, 딥러닝 학습은 이 손실 함수의 값을 최소화하는 가중치를 찾는 과정입니다.',
    },
    {
      id: 602,
      text: '현재 위치에서 기울기가 가장 가파른 방향으로 내려가며 최적의 가중치를 찾는 가장 기본적인 최적화 알고리즘은 무엇인가요?',
      options: ['경사하강법', '랜덤 서치', '그리드 서치', '유전 알고리즘'],
      answer: 0,
      shortExplanation: '가장 기본적인 방법은 경사하강법입니다.',
      detailedExplanation: '경사하강법은 손실 함수의 기울기(미분값)를 구하여 기울기가 낮은 쪽으로 가중치를 조금씩 업데이트하는 방식입니다.',
    },
    {
      id: 603,
      text: '경사하강법에 관성(Inertia)의 개념을 도입하여, 이전에 이동하던 방향으로 계속 가려는 성질을 이용해 학습 속도를 높인 알고리즘은 무엇인가요?',
      options: ['모멘텀', '드롭아웃', '정규화', '앙상블'],
      answer: 0,
      shortExplanation: '관성을 이용한 알고리즘은 모멘텀입니다.',
      detailedExplanation: '모멘텀은 중력 가속도처럼 과거의 기울기를 누적하여 가중치를 업데이트하므로, 경사하강법보다 빠르게 최적해에 도달할 수 있습니다.',
    },
    {
      id: 604,
      text: '기존 모멘텀 방식과 달리, 현재 속도로 이동한 지점의 기울기를 미리 계산(Look-ahead)하여 가중치를 업데이트함으로써 멈춰야 할 시점을 더 잘 예측하는 알고리즘은 무엇인가요?',
      options: ['네스테로프 모멘텀', '확률적 경사하강법', '배치 정규화', '가중치 감쇠'],
      answer: 0,
      shortExplanation: '미리 예측하여 움직이는 것은 네스테로프 모멘텀입니다.',
      detailedExplanation: '네스테로프 모멘텀은 관성으로 이동할 다음 위치를 미리 예측하고 그곳의 기울기를 사용하여 업데이트하므로, 일반 모멘텀보다 안정적으로 수렴합니다.',
    },
    {
      id: 605,
      text: '변수별로 학습률을 다르게 적용하는 적응적 학습(Adaptive Learning) 방법 중 하나로, 많이 변화한 변수의 학습률은 줄이고 적게 변화한 변수의 학습률은 높이는 방식은 무엇인가요?',
      options: ['AdaGrad', 'SGD', 'Momentum', 'Newton Method'],
      answer: 0,
      shortExplanation: '변수별 학습률 조절의 시초는 AdaGrad입니다.',
      detailedExplanation: 'AdaGrad는 과거의 기울기 제곱을 누적하여 학습률을 조절합니다. 하지만 학습이 진행될수록 학습률이 0에 가까워져 학습이 멈추는 단점이 있습니다.',
    },
    {
      id: 606,
      text: 'AdaGrad의 단점인 학습률이 0으로 수렴하는 문제를 해결하기 위해, 과거의 기울기를 지수이동평균(EMA) 방식으로 반영하는 알고리즘은 무엇인가요?',
      options: ['RMSProp', 'Leaky ReLU', 'Max Pooling', 'Softmax'],
      answer: 0,
      shortExplanation: 'AdaGrad의 단점을 보완한 것은 RMSProp입니다.',
      detailedExplanation: 'RMSProp은 과거의 모든 기울기를 균일하게 더하는 대신, 지수이동평균을 사용하여 최근 기울기를 더 크게 반영함으로써 학습률이 급격히 줄어드는 것을 방지합니다.',
    },
    {
      id: 607,
      text: '모멘텀의 관성 개념과 RMSProp의 적응적 학습률 개념을 합쳐 만든, 현재 가장 널리 사용되는 최적화 알고리즘은 무엇인가요?',
      options: ['Adam', 'AlexNet', 'VGG', 'ResNet'],
      answer: 0,
      shortExplanation: '두 장점을 합친 알고리즘은 Adam입니다.',
      detailedExplanation: 'Adam은 모멘텀의 속도와 RMSProp의 안정적인 학습률 조정을 모두 결합하여, 대부분의 딥러닝 문제에서 좋은 성능을 보이는 알고리즘입니다.',
    },
    {
      id: 608,
      text: 'Adam 알고리즘에서 가중치 감쇠(Weight Decay)를 손실 함수에 더하지 않고, 가중치 업데이트 식에 직접 적용하여 규제(Regularization) 효과를 정상화한 알고리즘은 무엇인가요?',
      options: ['AdamW', 'AdamX', 'Eve', 'Nadam'],
      answer: 0,
      shortExplanation: '가중치 감쇠를 분리한 것은 AdamW입니다.',
      detailedExplanation: '일반적인 Adam 구현에서는 L2 규제와 가중치 감쇠가 다르게 동작할 수 있는데, AdamW는 이를 분리하여 가중치 업데이트 시에 직접 감쇠를 적용함으로써 일반화 성능을 높였습니다.',
    }
  ]
};