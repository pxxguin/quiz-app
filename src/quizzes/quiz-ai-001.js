export default {
  id: 'quiz-ai-001',
  title: 'BERT 모델의 동작 원리',
  description: 'How BERT model operate in python?에 대한 퀴즈입니다. 해당 포스팅을 참고하여 문제를 풀어보세요.',
  author: 'Hong',
  category: '자연어 처리',
  points: 160,
  createdAt: '2025-11-24',
  questions: [
    {
      id: 201,
      text: '해당 포스팅에서 실습을 위해 사용한 모델로, 지식 증류 기법을 통해 기존 BERT를 경량화한 모델의 이름은 무엇인가요?',
      options: ['Albert', 'Roberta', 'DistilBERT', 'KoBERT'],
      answer: 2,
      shortExplanation: '정답은 DistilBERT입니다.',
      detailedExplanation: '포스팅에서는 로컬 환경에서도 부담 없이 실행할 수 있도록 지식 증류(Knowledge Distillation)를 기반으로 BERT를 경량화한 DistilBERT를 사용했습니다.',
    },
    {
      id: 202,
      text: '토큰화 과정에서 서로 다른 문맥(예: 기업 vs 과일)에서 쓰인 동일한 단어 Apple은 어떻게 처리되나요?',
      options: ['서로 다른 토큰 ID를 갖는다', '동일한 토큰 ID를 갖는다', '특수 토큰으로 변환된다', '문장의 길이에 따라 ID가 바뀐다'],
      answer: 1,
      shortExplanation: '문맥이 달라도 동일한 토큰 ID를 갖습니다.',
      detailedExplanation: '토큰화 단계에서는 단어 자체를 사전에 정의된 숫자로 바꿀 뿐이므로, 기업 Apple과 과일 Apple은 모두 동일한 6207이라는 토큰 ID를 가집니다.',
    },
    {
      id: 203,
      text: '토큰화 결과 중 문장의 길이를 맞추기 위해 사용되는 패딩 값을 의미하는 특수 토큰은 무엇인가요?',
      options: ['[CLS]', '[SEP]', '[MASK]', '[PAD]'],
      answer: 3,
      shortExplanation: '패딩을 나타내는 토큰은 [PAD]입니다.',
      detailedExplanation: '[PAD]는 길이가 다른 문장들의 길이를 맞춰주기 위해 사용되는 토큰으로, 해당 포스팅의 예시에서는 0이라는 값으로 표현되었습니다.',
    },
    {
      id: 204,
      text: '토큰화 결과 딕셔너리에서, 실제 의미가 있는 토큰과 패딩을 구분하기 위해 사용되는 것은 무엇인가요?',
      options: ['어텐션 마스크', '세그먼트 임베딩', '토큰 타입 ID', '풀링 레이어'],
      answer: 0,
      shortExplanation: '실제 토큰과 패딩을 구분하는 것은 어텐션 마스크입니다.',
      detailedExplanation: '어텐션 마스크(attention_mask)는 실제 단어가 있는 곳은 1, 패딩이 있는 곳은 0으로 표시하여 모델이 패딩 부분에 불필요하게 집중하지 않도록 돕습니다.',
    },
    {
      id: 205,
      text: 'BERT 모델이 동일한 토큰 ID를 가진 단어(Apple)의 의미를 문맥에 따라 다르게 해석할 수 있는 이유는 무엇인가요?',
      options: ['단어 사전의 정의를 참조하기 때문에', '주변 단어와의 집중도를 계산하기 때문에', '사용자가 직접 태그를 입력하기 때문에', '문장의 순서를 무시하기 때문에'],
      answer: 1,
      shortExplanation: '주변 단어와의 집중도(Attention)를 통해 문맥을 파악합니다.',
      detailedExplanation: '포스팅의 시각화 자료에서 볼 수 있듯이, 모델은 Apple이 Enterprise와 함께 쓰일 때는 기업으로, Eating과 함께 쓰일 때는 음식으로 인식하도록 주변 단어에 집중하여 문맥을 이해합니다.',
    },
    {
      id: 206,
      text: 'BERT 모델에서 문장의 시작과 끝을 나타내는 특수 토큰의 조합으로 올바른 것은 무엇인가요?',
      options: ['[START], [END]', '[CLS], [SEP]', '[BOS], [EOS]', '[HEAD], [TAIL]'],
      answer: 1,
      shortExplanation: '시작은 [CLS], 끝은 [SEP]입니다.',
      detailedExplanation: '포스팅 내용에 따르면 [CLS]는 문장의 시작을, [SEP]는 문장의 끝을 나타내는 특수 토큰입니다.',
    },
    {
      id: 207,
      text: '실습에 사용된 `distilbert-base-uncased` 모델 이름에서 `uncased`가 의미하는 특징은 무엇인가요?',
      options: ['대소문자를 구분한다', '모든 문자를 대문자로 변환한다', '모든 문자를 소문자로 변환하여 처리한다', '특수문자를 제거한다'],
      answer: 2,
      shortExplanation: 'uncased는 대소문자를 구분하지 않고 소문자로 처리함을 의미합니다.',
      detailedExplanation: '포스팅에서 "모든 토큰들은 소문자로 표현됨"이라고 언급했듯이, uncased 모델은 입력 텍스트를 소문자로 변환하여 토큰화를 진행합니다.',
    },
    {
      id: 208,
      text: 'Tokenizer를 통과한 결과(`tokenized_text`)에 포함된 키(key)로, 각 단어에 대응되는 정수 인덱스 배열을 담고 있는 것은 무엇인가요?',
      options: ['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values'],
      answer: 0,
      shortExplanation: '정답은 input_ids입니다.',
      detailedExplanation: '포스팅의 TIP 부분에 설명된 것처럼, input_ids는 하나의 문장을 각 단어들의 토큰값(숫자) 벡터로 표현한 것입니다.',
    }
  ]
};